{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import os\n",
    "import sys\n",
    "import torch\n",
    "import random\n",
    "import numpy as np\n",
    "np.set_printoptions(threshold=sys.maxsize)\n",
    "import torchvision\n",
    "from tqdm import tqdm\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import matplotlib as mpl\n",
    "mpl.rcParams['figure.dpi'] = 300\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn.functional as F\n",
    "from torchvision import transforms as T\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import Dataset\n",
    "from torchvision import datasets\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "from sklearn.metrics import classification_report\n",
    "from utils.one_shot import Prediction\n",
    "# from torchsummary import summary\n",
    "\n",
    "# from pytorch_metric_learning import losses\n",
    "# Lion Optimizer\n",
    "# from lion_pytorch import Lion\n",
    "\n",
    "\n",
    "# Utilities\n",
    "from utils.nutil import Utility\n",
    "from utils.persiandb import PersianDataset\n",
    "from utils.threshold_types import ThresholdType\n",
    "from utils.optim_acc import optim_accuracy\n",
    "from model.architecture import *\n",
    "\n",
    "# Load src/.env file\n",
    "from dotenv import load_dotenv\n",
    "import warnings\n",
    "load_dotenv()\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "random.seed(42)\n",
    "\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configurations\n",
    "main_db_path            = os.getenv('MAIN_DB_PATH')\n",
    "preprocessed_data_path  = os.getenv('PREPROCESSED_DATA_PATH')\n",
    "training_csv            = os.getenv('TRAINING_CSV')\n",
    "testing_csv             = os.getenv('TESTING_CSV')\n",
    "anchor_data_path        = os.getenv('ANCHOR_PATH')\n",
    "\n",
    "TRANSFORM_METHOD = T.Compose([T.ToTensor(), T.Resize(224)])\n",
    "BATCH_SIZE = 32\n",
    "LEARNING_RATE = 1e-5\n",
    "# WEIGHT_DECAY = 0.05\n",
    "STEPLR_GAMMA = 0.1\n",
    "EPOCHS = 35\n",
    "\n",
    "# For Utility class\n",
    "SAMPLES_PER_CLASS = 15\n",
    "THRESHOLD = ThresholdType.BINARY_INV\n",
    "RUN_UTILITY = True\n",
    "\n",
    "# Utility functiont to generate preprocessed images and csv files\n",
    "if RUN_UTILITY:\n",
    "    util = Utility(db_path=main_db_path,\n",
    "                   anchor_path=anchor_data_path,\n",
    "                   preprocessed_path=preprocessed_data_path,\n",
    "                   threshold_type=THRESHOLD)\n",
    "\n",
    "    util.load_db_per_image(images_per_character=SAMPLES_PER_CLASS, save_preprocessed=False, save_as_csv=False, shuffle=True, ratio = (1, 1))\n",
    "    parent_class, one_shot_anchor, _ = util.one_shot_db()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "one_shot_test = ['0641_26.tif']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Datasets\n",
    "persian_train = PersianDataset(training_csv, preprocessed_data_path, TRANSFORM_METHOD)\n",
    "persian_test  = PersianDataset(testing_csv, preprocessed_data_path, TRANSFORM_METHOD)\n",
    "\n",
    "# def seed_worker(worker_id):\n",
    "#     worker_seed = torch.initial_seed() % 2**32\n",
    "#     np.random.seed(worker_seed)\n",
    "#     random.seed(worker_seed)\n",
    "\n",
    "# g = torch.Generator()\n",
    "# g.manual_seed(42)\n",
    "\n",
    "train_dataloader = DataLoader(persian_train, shuffle=True, num_workers=0, batch_size=BATCH_SIZE)\n",
    "test_dataloader  = DataLoader(persian_test, shuffle=True, num_workers=0, batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n"
     ]
    }
   ],
   "source": [
    "# Check if GPU is available\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"mps\" if torch.backends.mps.is_available() else \"cpu\"\n",
    "print(f\"Using {device} device\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Contrastive Loss\n",
    "class ContrastiveLoss(nn.Module):\n",
    "    \"\"\"\n",
    "    Contrastive loss\n",
    "    Takes embeddings of two samples and a target label == 1 if samples are from the same class and label == 0 otherwise\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, margin):\n",
    "        super(ContrastiveLoss, self).__init__()\n",
    "        self.margin = margin\n",
    "        self.eps = 1e-9\n",
    "\n",
    "    def forward(self, output1, output2, target, size_average=True):\n",
    "        distances = (output2 - output1).pow(2).sum(1)  # squared distances\n",
    "        losses = .5 * (target.float() * distances +\n",
    "                        (1 + -1 * target).float() * F.relu(self.margin - (distances + self.eps).sqrt()).pow(2))\n",
    "        return losses.mean() if size_average else losses.sum()\n",
    "\n",
    "# class ContrastiveLoss(torch.nn.Module):\n",
    "#     \"\"\"\n",
    "#         Contrastive loss function.\n",
    "#         Based on: `\"Learning a Similarity Metric Discriminatively, with Application to Face Verification\" <http://yann.lecun.com/exdb/publis/pdf/hadsell-chopra-lecun-06.pdf>`_.\n",
    "#         Takes embeddings of two samples and a target label == 1 if samples are from the same class and label == 0 otherwise\n",
    "#     \"\"\"\n",
    "#     def __init__(self, margin=1.0):\n",
    "#         super(ContrastiveLoss, self).__init__()\n",
    "#         self.margin = margin\n",
    "\n",
    "#     def forward(self, output1, output2, label):\n",
    "#         euclidean_distance = F.pairwise_distance(output1, output2)\n",
    "#         loss_contrastive = torch.mean((1-label) * torch.pow(euclidean_distance, 2) +\n",
    "#                                       (label) * torch.pow(torch.clamp(self.margin - euclidean_distance, min=0.0), 2))\n",
    "#         return loss_contrastive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = CustomSiameseNetwork2().to(device)\n",
    "# criterion = nn.BCELoss()\n",
    "criterion = ContrastiveLoss(1.)\n",
    "optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "scheduler = StepLR(optimizer, step_size=5, gamma=STEPLR_GAMMA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "counter = []\n",
    "loss_train = []\n",
    "loss_test = []\n",
    "accuracy_train = []\n",
    "accuracy_test = []\n",
    "iteration_number = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, device, train_loader, optimizer, criterion, epoch, counter, iteration_number):\n",
    "    model.train()\n",
    "    \n",
    "    running_loss = 0.0\n",
    "    \n",
    "    train_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    # use tqdm\n",
    "    pbar = tqdm(train_loader)\n",
    "    for batch_idx, (images_1, images_2, targets) in enumerate(pbar):\n",
    "        images_1, images_2, targets = images_1.to(device), images_2.to(device), targets.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        output1, output2 = model(images_1, images_2)\n",
    "        \n",
    "        loss = criterion(output1, output2, targets)\n",
    "        train_loss += loss.item()\n",
    "        distances = F.pairwise_distance(output1, output2)\n",
    "        similarities = 1 - distances\n",
    "        predicted = (similarities >= 0.3).int()\n",
    "        correct += predicted.eq(targets).sum().item()\n",
    "        total += targets.size(0)\n",
    "\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "        \n",
    "        # add to counter and loss\n",
    "        if batch_idx % 100 == 99:\n",
    "            # print('Batch %d, Loss: %.4f' % (batch_idx+1, running_loss/100))\n",
    "            counter.append(iteration_number)\n",
    "            # loss_history.append(running_loss/100)\n",
    "            iteration_number += 100\n",
    "            running_loss = 0.0\n",
    "        \n",
    "        pbar.set_description(desc= f'Epoch {epoch} loss={loss.item()} batch_id={batch_idx}')\n",
    "    \n",
    "    train_loss /= len(train_loader.dataset)\n",
    "    accuracy = 100. * correct / total\n",
    "    accuracy_train.append(accuracy)\n",
    "    loss_train.append(train_loss)\n",
    "\n",
    "    print('Train set: Average loss: {:.4f}, Accuracy: {}/{} ({:.2f}%)'.format(\n",
    "        train_loss, correct, total, accuracy))\n",
    "\n",
    "# def train(model, device, train_loader, optimizer, criterion, epoch, counter, iteration_number):\n",
    "#     model.train()\n",
    "    \n",
    "#     train_loss = 0\n",
    "#     correct = 0\n",
    "#     total = 0\n",
    "#     # use tqdm\n",
    "#     pbar = tqdm(train_loader)\n",
    "#     for batch_idx, (images_1, images_2, targets) in enumerate(pbar):\n",
    "#         images_1, images_2, targets = images_1.to(device), images_2.to(device), targets.to(device)\n",
    "        \n",
    "#         optimizer.zero_grad()\n",
    "#         outputs = model(images_1, images_2).squeeze()\n",
    "        \n",
    "#         loss = criterion(outputs, targets)\n",
    "#         train_loss += loss.item()\n",
    "\n",
    "#         predicted = torch.where(outputs > 0.5, 1, 0)\n",
    "#         correct += predicted.eq(targets.view_as(predicted)).sum().item()\n",
    "#         total += targets.size(0)\n",
    "#         loss.backward()\n",
    "#         optimizer.step()\n",
    "        \n",
    "        \n",
    "#         # add to counter and loss\n",
    "#         if batch_idx % 100 == 99:\n",
    "#             # print('Batch %d, Loss: %.4f' % (batch_idx+1, running_loss/100))\n",
    "#             counter.append(iteration_number)\n",
    "#             # loss_history.append(running_loss/100)\n",
    "#             iteration_number += 100\n",
    "        \n",
    "#         pbar.set_description(desc= f'Epoch {epoch} loss={loss.item()} batch_id={batch_idx}')\n",
    "    \n",
    "#     train_loss /= len(train_loader.dataset)\n",
    "#     accuracy = 100. * correct / total\n",
    "#     accuracy_train.append(accuracy)\n",
    "#     loss_train.append(train_loss)\n",
    "\n",
    "#     print('Train set: Average loss: {:.4f}, Accuracy: {}/{} ({:.2f}%)'.format(\n",
    "#         train_loss, correct, total, accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def test(model, device, test_loader, criterion):\n",
    "#     model.eval()\n",
    "#     test_loss = 0\n",
    "#     correct = 0\n",
    "    \n",
    "#     # use tqdm\n",
    "#     pbar = tqdm(test_loader)\n",
    "#     with torch.no_grad():\n",
    "#         for (images_1, images_2, targets) in pbar:\n",
    "#             images_1, images_2, targets = images_1.to(device), images_2.to(device), targets.to(device)\n",
    "#             outputs = model(images_1, images_2).squeeze()\n",
    "#             test_loss += criterion(outputs, targets).item()\n",
    "#             pred = torch.where(outputs > 0.5, 1, 0)\n",
    "#             correct += pred.eq(targets.view_as(pred)).sum().item()\n",
    "            \n",
    "#             pbar.set_description(desc= f'test_loss={test_loss}')\n",
    "#     test_loss /= len(test_loader.dataset)\n",
    "#     print(f'Average loss: {test_loss:.4f}, Accuracy: {correct}/{len(test_loader.dataset)} ({100. * correct / len(test_loader.dataset):.4f}%)')\n",
    "\n",
    "def test(model, device, test_loader, criterion):\n",
    "\n",
    "    # Set the model to evaluation mode\n",
    "    model.eval()\n",
    "\n",
    "    # Initialize variables to keep track of the loss and accuracy\n",
    "    test_loss = 0\n",
    "    # correct = 0\n",
    "    total = 0\n",
    "    similarity_collection, target_collection = [], []\n",
    "\n",
    "    \n",
    "\n",
    "    # Use tqdm to display the progress of the testing loop\n",
    "    with tqdm(total=len(test_loader.dataset)) as pbar:\n",
    "        with torch.no_grad():\n",
    "            for batch_idx, (images_1, images_2, targets) in enumerate(test_loader):\n",
    "                # Move the data and target to the GPU\n",
    "                images_1, images_2, targets = images_1.to(device), images_2.to(device), targets.to(device)\n",
    "\n",
    "                # Forward pass\n",
    "                output1, output2 = model(images_1, images_2)\n",
    "                loss = criterion(output1, output2, targets)\n",
    "\n",
    "                # Update loss and accuracy\n",
    "                test_loss += loss.item()\n",
    "                distances = F.pairwise_distance(output1, output2)\n",
    "                similarities = 1 - distances\n",
    "\n",
    "                target_collection.extend(targets.cpu().numpy())\n",
    "                similarity_collection.extend(similarities.cpu().numpy())\n",
    "                # predicted = (similarities >= 0.3).int()\n",
    "                # correct += predicted.eq(targets).sum().item()\n",
    "                total += targets.size(0)\n",
    "\n",
    "                # Update tqdm progress bar\n",
    "                pbar.update(images_1.size(0))\n",
    "    \n",
    "    correct, accuracy, threshold = optim_accuracy(similarity_collection, target_collection)\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    accuracy_test.append(accuracy)\n",
    "    loss_test.append(test_loss)\n",
    "\n",
    "    # Print the results\n",
    "    print('Test set: Average loss: {:.4f}, Accuracy: {}/{} ({:.2f}%)'.format(\n",
    "        test_loss, correct, total, accuracy))\n",
    "\n",
    "    return test_loss, accuracy\n",
    "\n",
    "# def test(model, device, test_loader, criterion):\n",
    "#     # Set the model to evaluation mode\n",
    "#     model.eval()\n",
    "\n",
    "#     # Initialize variables to keep track of the loss and accuracy\n",
    "#     test_loss = 0\n",
    "#     # correct = 0\n",
    "#     total = 0\n",
    "#     similarity_collection, target_collection = [], []\n",
    "\n",
    "    \n",
    "\n",
    "#     # Use tqdm to display the progress of the testing loop\n",
    "#     with tqdm(total=len(test_loader.dataset)) as pbar:\n",
    "#         with torch.no_grad():\n",
    "#             for batch_idx, (images_1, images_2, targets) in enumerate(test_loader):\n",
    "#                 # Move the data and target to the device\n",
    "#                 images_1, images_2, targets = images_1.to(device), images_2.to(device), targets.to(device)\n",
    "\n",
    "#                 # Forward pass\n",
    "#                 outputs = model(images_1, images_2).squeeze()\n",
    "#                 loss = criterion(outputs, targets)\n",
    "\n",
    "#                 # Update loss and accuracy\n",
    "#                 test_loss += loss.item()\n",
    "\n",
    "#                 target_collection.extend(targets.cpu().numpy())\n",
    "#                 similarity_collection.extend(outputs.cpu().numpy())\n",
    "#                 # predicted = (similarities >= 0.3).int()\n",
    "#                 # correct += predicted.eq(targets).sum().item()\n",
    "#                 total += targets.size(0)\n",
    "\n",
    "#                 # Update tqdm progress bar\n",
    "#                 pbar.update(images_1.size(0))\n",
    "    \n",
    "#     min_val, max_val = np.min(similarity_collection), np.max(similarity_collection)\n",
    "#     acc_collection = []\n",
    "#     correct_collection = []\n",
    "\n",
    "#     for val in np.arange(min_val, max_val, 0.001):\n",
    "#         temp = np.array(similarity_collection) > val\n",
    "#         correct = np.sum(temp == np.array(target_collection))\n",
    "#         correct_collection.append(correct)\n",
    "#         acc_collection.append(100 * correct / len(similarity_collection))\n",
    "\n",
    "#     # Compute the average loss and accuracy\n",
    "#     test_loss /= len(test_loader.dataset)\n",
    "#     # accuracy = 100. * correct / total\n",
    "#     accuracy = np.max(acc_collection)\n",
    "#     accuracy_test.append(accuracy)\n",
    "#     loss_test.append(test_loss)\n",
    "\n",
    "#     # Print the results\n",
    "#     print('Test set: Average loss: {:.4f}, Accuracy: {}/{} ({:.2f}%)'.format(\n",
    "#         test_loss, np.max(correct_collection), total, accuracy))\n",
    "\n",
    "#     return test_loss, accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/313 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Input type (c10::Half) and bias type (float) should be the same",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39mfor\u001b[39;00m epoch \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m1\u001b[39m, EPOCHS \u001b[39m+\u001b[39m \u001b[39m1\u001b[39m):\n\u001b[0;32m----> 2\u001b[0m     train(model, device, train_dataloader, optimizer, criterion, epoch, counter, iteration_number)\n\u001b[1;32m      3\u001b[0m     \u001b[39m# plot_loss()\u001b[39;00m\n\u001b[1;32m      4\u001b[0m     test(model, device, test_dataloader, criterion)\n",
      "Cell \u001b[0;32mIn[9], line 15\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(model, device, train_loader, optimizer, criterion, epoch, counter, iteration_number)\u001b[0m\n\u001b[1;32m     12\u001b[0m images_1, images_2, targets \u001b[39m=\u001b[39m images_1\u001b[39m.\u001b[39mto(device), images_2\u001b[39m.\u001b[39mto(device), targets\u001b[39m.\u001b[39mto(device)\n\u001b[1;32m     14\u001b[0m optimizer\u001b[39m.\u001b[39mzero_grad()\n\u001b[0;32m---> 15\u001b[0m output1, output2 \u001b[39m=\u001b[39m model(images_1, images_2)\n\u001b[1;32m     17\u001b[0m loss \u001b[39m=\u001b[39m criterion(output1, output2, targets)\n\u001b[1;32m     18\u001b[0m train_loss \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m loss\u001b[39m.\u001b[39mitem()\n",
      "File \u001b[0;32m/media/blacfli/Micblacfli/Michael Evan Santoso/Persian_OCR/Research/Persian_Historical_Document_Digitalizing/applied/persian-docs-ocr/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m/media/blacfli/Micblacfli/Michael Evan Santoso/Persian_OCR/Research/Persian_Historical_Document_Digitalizing/applied/persian-docs-ocr/src/model/architecture.py:178\u001b[0m, in \u001b[0;36mCustomSiameseNetwork2.forward\u001b[0;34m(self, x1, x2)\u001b[0m\n\u001b[1;32m    177\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, x1, x2):\n\u001b[0;32m--> 178\u001b[0m     output1 \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mforward_once(x1)\n\u001b[1;32m    179\u001b[0m     output2 \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mforward_once(x2)\n\u001b[1;32m    180\u001b[0m     \u001b[39mreturn\u001b[39;00m output1, output2\n",
      "File \u001b[0;32m/media/blacfli/Micblacfli/Michael Evan Santoso/Persian_OCR/Research/Persian_Historical_Document_Digitalizing/applied/persian-docs-ocr/src/model/architecture.py:171\u001b[0m, in \u001b[0;36mCustomSiameseNetwork2.forward_once\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    169\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward_once\u001b[39m(\u001b[39mself\u001b[39m, x):\n\u001b[1;32m    170\u001b[0m     \u001b[39m# Convert grayscale images to RGB images\u001b[39;00m\n\u001b[0;32m--> 171\u001b[0m     output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mconvnet(x)\n\u001b[1;32m    172\u001b[0m     output \u001b[39m=\u001b[39m output\u001b[39m.\u001b[39mview(output\u001b[39m.\u001b[39msize()[\u001b[39m0\u001b[39m], \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[1;32m    173\u001b[0m     output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfc(output)\n",
      "File \u001b[0;32m/media/blacfli/Micblacfli/Michael Evan Santoso/Persian_OCR/Research/Persian_Historical_Document_Digitalizing/applied/persian-docs-ocr/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m/media/blacfli/Micblacfli/Michael Evan Santoso/Persian_OCR/Research/Persian_Historical_Document_Digitalizing/applied/persian-docs-ocr/.venv/lib/python3.10/site-packages/torch/nn/modules/container.py:217\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    215\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m):\n\u001b[1;32m    216\u001b[0m     \u001b[39mfor\u001b[39;00m module \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m:\n\u001b[0;32m--> 217\u001b[0m         \u001b[39minput\u001b[39m \u001b[39m=\u001b[39m module(\u001b[39minput\u001b[39;49m)\n\u001b[1;32m    218\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39minput\u001b[39m\n",
      "File \u001b[0;32m/media/blacfli/Micblacfli/Michael Evan Santoso/Persian_OCR/Research/Persian_Historical_Document_Digitalizing/applied/persian-docs-ocr/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m/media/blacfli/Micblacfli/Michael Evan Santoso/Persian_OCR/Research/Persian_Historical_Document_Digitalizing/applied/persian-docs-ocr/.venv/lib/python3.10/site-packages/torch/nn/modules/conv.py:463\u001b[0m, in \u001b[0;36mConv2d.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    462\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[0;32m--> 463\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_conv_forward(\u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias)\n",
      "File \u001b[0;32m/media/blacfli/Micblacfli/Michael Evan Santoso/Persian_OCR/Research/Persian_Historical_Document_Digitalizing/applied/persian-docs-ocr/.venv/lib/python3.10/site-packages/torch/nn/modules/conv.py:459\u001b[0m, in \u001b[0;36mConv2d._conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    455\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpadding_mode \u001b[39m!=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mzeros\u001b[39m\u001b[39m'\u001b[39m:\n\u001b[1;32m    456\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39mconv2d(F\u001b[39m.\u001b[39mpad(\u001b[39minput\u001b[39m, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpadding_mode),\n\u001b[1;32m    457\u001b[0m                     weight, bias, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstride,\n\u001b[1;32m    458\u001b[0m                     _pair(\u001b[39m0\u001b[39m), \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdilation, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgroups)\n\u001b[0;32m--> 459\u001b[0m \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mconv2d(\u001b[39minput\u001b[39;49m, weight, bias, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mstride,\n\u001b[1;32m    460\u001b[0m                 \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpadding, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdilation, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgroups)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Input type (c10::Half) and bias type (float) should be the same"
     ]
    }
   ],
   "source": [
    "for epoch in range(1, EPOCHS + 1):\n",
    "    train(model, device, train_dataloader, optimizer, criterion, epoch, counter, iteration_number)\n",
    "    # plot_loss()\n",
    "    test(model, device, test_dataloader, criterion)\n",
    "    scheduler.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.save(model.state_dict(), 'trained_model/customSNN_10sp_final_1.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_state_dict(torch.load('trained_model/customSNN_15sp_final_1.pth'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'test_dataloader' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m test(model, device, test_dataloader, criterion)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'test_dataloader' is not defined"
     ]
    }
   ],
   "source": [
    "test(model, device, test_dataloader, criterion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:12<00:00, 12.81s/it]\n"
     ]
    }
   ],
   "source": [
    "OneShotPredictor = Prediction(model, one_shot_anchor, one_shot_test, TRANSFORM_METHOD, preprocessed_data_path, device, parent_class = parent_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "64"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.argmax(list(OneShotPredictor._one_shot_result['fecb_26.tif'].values()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'0627': -40.10827445983887,\n",
       " 'A': -33.01774978637695,\n",
       " '06400627': -30.006999492645264,\n",
       " '0628': -31.694190979003906,\n",
       " 'fe90': -33.127503871917725,\n",
       " 'fe92': -32.588833808898926,\n",
       " 'fe91': -28.748319625854492,\n",
       " 'fe96': -21.175569891929626,\n",
       " '062a': -20.645509123802185,\n",
       " 'fe97': -27.141194581985474,\n",
       " 'fe98': -18.29543375968933,\n",
       " '062b': -16.837021112442017,\n",
       " 'fe9a': -18.552802681922913,\n",
       " 'fe9b': -23.397119760513306,\n",
       " 'fe9c': -15.973437905311584,\n",
       " 'fe9f': -27.630797147750854,\n",
       " 'fea0': -26.679434061050415,\n",
       " 'fe9e': -12.42771589756012,\n",
       " '062c': -14.874951958656311,\n",
       " 'fea2': -23.052621126174927,\n",
       " 'fea4': -7.568445205688477,\n",
       " '062d': -24.32414174079895,\n",
       " 'fea3': -13.407267928123474,\n",
       " 'fea7': -15.632276058197021,\n",
       " 'fea8': -6.581384837627411,\n",
       " '062e': -17.44503402709961,\n",
       " 'fea6': -15.529524326324463,\n",
       " '062f': -30.79301428794861,\n",
       " '0640062f': -25.877999305725098,\n",
       " '06400630': -19.530291199684143,\n",
       " '0630': -31.18294858932495,\n",
       " '0631': -30.43973994255066,\n",
       " '06400631': -31.17378878593445,\n",
       " '0632': -27.4537250995636,\n",
       " '06400632': -21.286429524421692,\n",
       " 'feb2': -30.157941102981567,\n",
       " '0633': -31.50416326522827,\n",
       " 'feb3': -24.11551284790039,\n",
       " 'feb4': -26.552088975906372,\n",
       " 'feb6': -30.475462913513184,\n",
       " 'feb7': -18.146637797355652,\n",
       " 'feb8': -16.868581533432007,\n",
       " '0634': -25.424034118652344,\n",
       " 'febb': -17.53370451927185,\n",
       " '0635': -39.61036658287048,\n",
       " 'febc': -17.787300944328308,\n",
       " 'feba': -36.42445635795593,\n",
       " 'fec0': -17.55539643764496,\n",
       " '0636': -35.72729134559631,\n",
       " 'febe': -32.73486804962158,\n",
       " 'febf': -19.386098861694336,\n",
       " 'fec2': -21.09378671646118,\n",
       " 'fec4': -20.041428208351135,\n",
       " 'fec3': -21.651907444000244,\n",
       " '0637': -22.215736865997314,\n",
       " '0638': -17.94755756855011,\n",
       " 'fec6': -15.886391520500183,\n",
       " 'fec8': -16.114945769309998,\n",
       " 'fec7': -16.89414095878601,\n",
       " 'feca': -21.64011836051941,\n",
       " '0639': -26.796231746673584,\n",
       " 'fecb': -5.993362605571747,\n",
       " 'fecc': -14.872939705848694,\n",
       " 'fece': -21.094507932662964,\n",
       " 'fecf': -3.0653934478759766,\n",
       " '063a': -26.584168672561646,\n",
       " 'fed0': -18.130825519561768,\n",
       " 'fed4': -16.294570207595825,\n",
       " 'fed2': -17.37298822402954,\n",
       " 'fed3': -25.212042093276978,\n",
       " '0641': -18.768964767456055,\n",
       " 'fed8': -15.551949977874756,\n",
       " '0642': -20.736383199691772,\n",
       " 'fed6': -21.96838402748108,\n",
       " 'fed7': -25.549840211868286,\n",
       " 'fedf': -35.75815486907959,\n",
       " 'fee0': -27.997618198394775,\n",
       " '0644': -27.22608184814453,\n",
       " 'fede': -25.57817006111145,\n",
       " 'fee3': -26.397488832473755,\n",
       " 'fee4': -24.432698011398315,\n",
       " '0645': -37.5979278087616,\n",
       " 'fee2': -38.469385385513306,\n",
       " 'fee7': -19.663880705833435,\n",
       " 'fee6': -15.628100156784058,\n",
       " '0646': -18.808279037475586,\n",
       " 'fee8': -11.378299713134766,\n",
       " 'feeb': -17.204405546188354,\n",
       " 'feec': -24.854028463363647,\n",
       " 'feea': -26.741102695465088,\n",
       " '0647': -20.671395778656006,\n",
       " '06400648': -22.37129855155945,\n",
       " '0648': -22.844473600387573,\n",
       " 'fb58': -16.33932650089264,\n",
       " 'fb59': -15.972888946533203,\n",
       " 'fb57': -21.19723606109619,\n",
       " '067e': -23.865164518356323,\n",
       " 'fb7b': -8.61453664302826,\n",
       " 'fb7d': -11.505051851272583,\n",
       " '0686': -7.354026973247528,\n",
       " 'fb7c': -14.92188584804535,\n",
       " '06400698': -16.968887448310852,\n",
       " '0698': -23.776548862457275,\n",
       " '06a9': -29.00377321243286,\n",
       " 'fedc': -22.61296796798706,\n",
       " 'fedb': -28.250529289245605,\n",
       " 'feda': -19.521680235862732,\n",
       " 'fbfe': -19.57353925704956,\n",
       " 'fbff': -22.00895369052887,\n",
       " 'fbfd': -13.819074392318726,\n",
       " '06cc': -14.85078227519989}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "OneShotPredictor._one_shot_result['fecb_26.tif']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'fecf'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(OneShotPredictor._one_shot_result['fecb_26.tif'].keys())[64]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fecb       0.00      0.00      0.00       1.0\n",
      "        fecf       0.00      0.00      0.00       0.0\n",
      "\n",
      "    accuracy                           0.00       1.0\n",
      "   macro avg       0.00      0.00      0.00       1.0\n",
      "weighted avg       0.00      0.00      0.00       1.0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# print(OneShotPredictor.plot_confusion(contextual=False))\n",
    "print(OneShotPredictor.make_classification_report(contextual=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Object successfully saved to \"one_shot_pred_15sp_10epoch_contextual.pkl\"\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "file_name = 'one_shot_pred_15sp_10epoch_contextual.pkl'\n",
    "with open(file_name, 'wb') as file:\n",
    "    pickle.dump(OneShotPredictor, file)\n",
    "    print(f'Object successfully saved to \"{file_name}\"')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "file_name = 'one_shot_pred_15sp_10epoch.pkl'\n",
    "with open(file_name, 'rb') as file:\n",
    "    OneShotPredictor = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.8490516274164828, 0.8396396396396396, 0.8414155821885497, None)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "precision_recall_fscore_support(OneShotPredictor._y_true, OneShotPredictor._y_pred, average='weighted')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# result = one_shot_prediction(model, one_shot_anchor, one_shot_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import operator\n",
    "# from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, classification_report, accuracy_score\n",
    "# y_true = []\n",
    "# y_pred = []\n",
    "# label = list(list(result.values())[0].keys())\n",
    "# for key, value in result.items():\n",
    "#     y_true.append(key[:-7])\n",
    "#     y_pred.append(max(value.items(), key=operator.itemgetter(1))[0])\n",
    "#     # print(key[:-7], max(value.items(), key=operator.itemgetter(1))[0])\n",
    "# # cm = confusion_matrix(y_true, y_pred, labels=label)\n",
    "# # disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=label)\n",
    "# # disp.plot()\n",
    "# # plt.show()\n",
    "# print(classification_report(y_true, y_pred))\n",
    "# print(accuracy_score(y_true, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "o1, o2 = model(test_dataloader.dataset[0][0].unsqueeze(dim = -4).to(device), test_dataloader.dataset[0][1].unsqueeze(dim = -4).to(device))\n",
    "simil = 1 - F.pairwise_distance(o1, o2)\n",
    "simil.detach().cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_samples(model, test_loader, device):\n",
    "    # Set the model to evaluation mode\n",
    "    model.eval()\n",
    "\n",
    "    # Select 10 random pairs from the test dataloader\n",
    "    num_samples = min(10, len(test_loader.dataset))\n",
    "    sample_indices = np.random.choice(len(test_loader.dataset), size=num_samples, replace=False)\n",
    "    sample_images_1 = []\n",
    "    sample_images_2 = []\n",
    "    sample_targets = []\n",
    "    sample_predictions = []\n",
    "\n",
    "    for idx in sample_indices:\n",
    "        images_1, images_2, targets = test_loader.dataset[idx]\n",
    "        sample_images_1.append(images_1)\n",
    "        sample_images_2.append(images_2)\n",
    "        sample_targets.append(targets)\n",
    "\n",
    "        # Move the images to the device and perform the forward pass\n",
    "        images_1, images_2 = images_1.unsqueeze(0).to(device), images_2.unsqueeze(0).to(device)\n",
    "        with torch.no_grad():\n",
    "            output1, output2 = model(images_1, images_2)\n",
    "        distance = F.pairwise_distance(output1, output2)\n",
    "        similarity = 1 - distance\n",
    "        # print(similarity)\n",
    "        # temp = torch.where(similarity > 0., 1, 0)\n",
    "        sample_predictions.append(similarity.item())\n",
    "\n",
    "    # Create a grid of the sampled pairs and their labels\n",
    "    fig, axes = plt.subplots(nrows=5, ncols=2, figsize=(10, 12))\n",
    "    fig.suptitle('Sample Test Pairs', fontsize=16)\n",
    "\n",
    "    for i, ax in enumerate(axes.flatten()):\n",
    "        # Plot the images side by side\n",
    "        if i < num_samples:\n",
    "            image_1 = sample_images_1[i].numpy().transpose((1, 2, 0))\n",
    "            image_2 = sample_images_2[i].numpy().transpose((1, 2, 0))\n",
    "            ax.imshow(np.concatenate((image_1, image_2), axis=1))\n",
    "            ax.axis('off')\n",
    "\n",
    "            # Show the actual and predicted labels\n",
    "            actual_label = sample_targets[i].item()\n",
    "            similarity =  sample_predictions[i]\n",
    "            ax.set_title(f'Actual: {actual_label}\\n Similarity: {similarity:.2f}')\n",
    "\n",
    "        else:\n",
    "            # Hide the empty subplot\n",
    "            ax.axis('off')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "show_samples(model, test_dataloader, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(range(1, len(accuracy_train) + 1), accuracy_train, label = 'Training')\n",
    "plt.plot(range(1, len(accuracy_test) + 1), accuracy_test, label = 'Testing')\n",
    "plt.xlabel('Epochs', fontweight = 'bold', fontsize = 24)\n",
    "plt.ylabel('Accuracy', fontweight = 'bold', fontsize = 24)\n",
    "plt.legend()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Contrastive Loss\n",
    "#### 1. (10 Samples per class)\n",
    "a) Ratio (1:1)\n",
    "- Output: 10\n",
    "    > Accuracy : 86.90%\n",
    "\n",
    "- Output: 40\n",
    "    > Accuracy : 88.45%\n",
    "\n",
    "- Output: 64\n",
    "    > Accuracy : 88.62%\n",
    "\n",
    "- Output: 256\n",
    "    > Accuracy : 77.64%\n",
    "\n",
    "b) Ratio (1:2)\n",
    "\n",
    "- Output: 256\n",
    "    > Accuracy : 87.94%\n",
    "\n",
    "c) Ratio (1:3)\n",
    "\n",
    "- Output: 256\n",
    "    > Accuracy : 87.63%\n",
    "\n",
    "#### 1. (5 Samples per class)\n",
    "a) Ratio (1:1)\n",
    "- Output: 10\n",
    "    > Accuracy : \n",
    "\n",
    "- Output: 40\n",
    "    > Accuracy : \n",
    "\n",
    "- Output: 64\n",
    "    > Accuracy : \n",
    "\n",
    "- Output: 256\n",
    "    > Accuracy : 79.54%\n",
    "\n",
    "b) Ratio (1:2)\n",
    "\n",
    "- Output: 256\n",
    "    > Accuracy : 82.26%\n",
    "\n",
    "\n",
    "#### 1. (15 Samples per class)\n",
    "a) Ratio (1:1)\n",
    "- Output: 10\n",
    "    > Accuracy :\n",
    "\n",
    "- Output: 40\n",
    "    > Accuracy : \n",
    "\n",
    "- Output: 64\n",
    "    > Accuracy : \n",
    "\n",
    "- Output: 256\n",
    "    > Accuracy : 77.80%\n",
    "\n",
    "b) Ratio (1:2)\n",
    "\n",
    "- Output: 256\n",
    "    > Accuracy : 87.94%\n",
    "\n",
    "c) Ratio (1:3)\n",
    "\n",
    "- Output: 256\n",
    "    > Accuracy : 87.63%\n",
    "\n",
    "### BCE Loss\n",
    "- Resnet18 (Pretrained = True)\n",
    "    > Accuracy : 81.80%\n",
    "\n",
    "- Resnet50 (Pretrained = True)\n",
    "    > Accuracy : 79.43%\n",
    "\n",
    "- Resnet101 (Pretrained = True)\n",
    "    > Accuracy : 84.37%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "vscode": {
   "interpreter": {
    "hash": "6c8878839801fc59532b9802929b8590619fd74b7511d0e8f31397e46c60ce79"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
